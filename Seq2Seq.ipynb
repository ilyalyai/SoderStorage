{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Seq2Seq.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1mR1IzYmy3cHn-e4L50XrdjrUd4rC4Ms9",
      "authorship_tag": "ABX9TyP/hNTbcHRsAhhYRKTMgX5j",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ilyalyai/SoderStorage/blob/main/Seq2Seq.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%tensorflow_version 1.x"
      ],
      "metadata": {
        "id": "evqfz7ThyFZC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6d1ab58-a668-450f-fca8-91b854293d2a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow 1.x selected.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__, tf.test.is_gpu_available())\n",
        "import numpy as np\n",
        "import sys\n",
        "from random import randint\n",
        "import datetime\n",
        "from sklearn.utils import shuffle\n",
        "import pickle\n",
        "import os\n",
        "import warnings\n",
        "from google.colab import drive\n",
        "from google.colab import output\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=Warning)\n",
        "# Removes an annoying Tensorflow warning\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL']='2'\n",
        "drive.mount(\"/content/gdrive\")\n",
        "path = \"/content/gdrive/My Drive/Seq2Seq/\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yvDk2Y6J1x9_",
        "outputId": "36ba388d-d48a-4e6c-cfa8-aa45e689a1d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.15.2 True\n",
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def createTrainingMatrices(conversationFileName, wList, maxLen):\n",
        "    conversationDictionary = np.load(conversationFileName, allow_pickle=True).item()\n",
        "    numExamples = len(conversationDictionary)\n",
        "    xTrain = np.zeros((numExamples, maxLen), dtype='int32')\n",
        "    yTrain = np.zeros((numExamples, maxLen), dtype='int32')\n",
        "    for index,(key,value) in enumerate(conversationDictionary.items()):\n",
        "        # Will store integerized representation of strings here (initialized as padding)\n",
        "        encoderMessage = np.full((maxLen), wList.index('<pad>'), dtype='int32')\n",
        "        decoderMessage = np.full((maxLen), wList.index('<pad>'), dtype='int32')\n",
        "        # Getting all the individual words in the strings\n",
        "        keySplit = key.split()\n",
        "        valueSplit = value.split()\n",
        "        keyCount = len(keySplit)\n",
        "        valueCount = len(valueSplit)\n",
        "        # Throw out sequences that are too long or are empty\n",
        "        if (keyCount > (maxLen - 1) or valueCount > (maxLen - 1) or valueCount == 0 or keyCount == 0):\n",
        "            continue\n",
        "        # Integerize the encoder string\n",
        "        for keyIndex, word in enumerate(keySplit):\n",
        "            try:\n",
        "                encoderMessage[keyIndex] = wList.index(word)\n",
        "            except ValueError:\n",
        "                # TODO: This isnt really the right way to handle this scenario\n",
        "                encoderMessage[keyIndex] = 0\n",
        "        encoderMessage[keyIndex + 1] = wList.index('<EOS>')\n",
        "        # Integerize the decoder string\n",
        "        for valueIndex, word in enumerate(valueSplit):\n",
        "            try:\n",
        "                decoderMessage[valueIndex] = wList.index(word)\n",
        "            except ValueError:\n",
        "                decoderMessage[valueIndex] = 0\n",
        "        decoderMessage[valueIndex + 1] = wList.index('<EOS>')\n",
        "        xTrain[index] = encoderMessage\n",
        "        yTrain[index] = decoderMessage\n",
        "    # Remove rows with all zeros\n",
        "    yTrain = yTrain[~np.all(yTrain == 0, axis=1)]\n",
        "    xTrain = xTrain[~np.all(xTrain == 0, axis=1)]\n",
        "    numExamples = xTrain.shape[0]\n",
        "    return numExamples, xTrain, yTrain\n",
        "\n",
        "def getTrainingBatch(localXTrain, localYTrain, localBatchSize, maxLen):\n",
        "    num = randint(0,numTrainingExamples - localBatchSize - 1)\n",
        "    arr = localXTrain[num:num + localBatchSize]\n",
        "    labels = localYTrain[num:num + localBatchSize]\n",
        "    # Reversing the order of encoder string apparently helps as per 2014 paper\n",
        "    reversedList = list(arr)\n",
        "    for index,example in enumerate(reversedList):\n",
        "        reversedList[index] = list(reversed(example))\n",
        "\n",
        "    # Lagged labels are for the training input into the decoder\n",
        "    laggedLabels = []\n",
        "    EOStokenIndex = wordList.index('<EOS>')\n",
        "    padTokenIndex = wordList.index('<pad>')\n",
        "    for example in labels:\n",
        "        eosFound = np.argwhere(example==EOStokenIndex)[0]\n",
        "        shiftedExample = np.roll(example,1)\n",
        "        shiftedExample[0] = EOStokenIndex\n",
        "        # The EOS token was already at the end, so no need for pad\n",
        "        if (eosFound != (maxLen - 1)):\n",
        "            shiftedExample[eosFound+1] = padTokenIndex\n",
        "        laggedLabels.append(shiftedExample)\n",
        "\n",
        "    # Need to transpose these\n",
        "    reversedList = np.asarray(reversedList).T.tolist()\n",
        "    labels = labels.T.tolist()\n",
        "    laggedLabels = np.asarray(laggedLabels).T.tolist()\n",
        "    return reversedList, labels, laggedLabels\n",
        "\n",
        "def translateToSentences(inputs, wList, encoder=False):\n",
        "    EOStokenIndex = wList.index('<EOS>')\n",
        "    padTokenIndex = wList.index('<pad>')\n",
        "    numStrings = len(inputs[0])\n",
        "    numLengthOfStrings = len(inputs)\n",
        "    listOfStrings = [''] * numStrings\n",
        "    for mySet in inputs:\n",
        "        for index,num in enumerate(mySet):\n",
        "            if (num != EOStokenIndex and num != padTokenIndex):\n",
        "                if (encoder):\n",
        "                    # Encodings are in reverse!\n",
        "                    listOfStrings[index] = wList[num] + \" \" + listOfStrings[index]\n",
        "                else:\n",
        "                    listOfStrings[index] = listOfStrings[index] + \" \" + wList[num]\n",
        "    listOfStrings = [string.strip() for string in listOfStrings]\n",
        "    return listOfStrings\n",
        "\n",
        "def getTestInput(inputMessage, wList, maxLen):\n",
        "    encoderMessage = np.full((maxLen), wList.index('<pad>'), dtype='int32')\n",
        "    inputSplit = inputMessage.lower().split()\n",
        "    for index,word in enumerate(inputSplit):\n",
        "        try:\n",
        "            encoderMessage[index] = wList.index(word)\n",
        "        except ValueError:\n",
        "            continue\n",
        "    encoderMessage[index + 1] = wList.index('<EOS>')\n",
        "    encoderMessage = encoderMessage[::-1]\n",
        "    encoderMessageList=[]\n",
        "    for num in encoderMessage:\n",
        "        encoderMessageList.append([num])\n",
        "    return encoderMessageList\n",
        "\n",
        "def idsToSentence(ids, wList):\n",
        "    EOStokenIndex = wList.index('<EOS>')\n",
        "    padTokenIndex = wList.index('<pad>')\n",
        "    myStr = \"\"\n",
        "    listOfResponses=[]\n",
        "    for num in ids:\n",
        "        if (num[0] == EOStokenIndex or num[0] == padTokenIndex):\n",
        "            listOfResponses.append(myStr)\n",
        "            myStr = \"\"\n",
        "        else:\n",
        "            myStr = myStr + wList[num[0]] + \" \"\n",
        "    if myStr:\n",
        "        listOfResponses.append(myStr)\n",
        "    listOfResponses = [i for i in listOfResponses if i]\n",
        "    return listOfResponses"
      ],
      "metadata": {
        "id": "3Dn_oP7cy00x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparamters\n",
        "batchSize = 24\n",
        "maxEncoderLength = 15\n",
        "maxDecoderLength = maxEncoderLength\n",
        "lstmUnits = 112\n",
        "embeddingDim = lstmUnits\n",
        "numLayersLSTM = 3\n",
        "numIterations = 500000\n",
        "\n",
        "# Loading in all the data structures\n",
        "with open(path + \"wordList.txt\", \"rb\") as fp:\n",
        "    wordList = pickle.load(fp)\n",
        "\n",
        "vocabSize = len(wordList)\n",
        "\n",
        "# If you've run the entirety of word2vec.py then these lines will load in\n",
        "# the embedding matrix.\n",
        "if (os.path.isfile(path + 'embeddingMatrix.npy')):\n",
        "    wordVectors = np.load(path + 'embeddingMatrix.npy', allow_pickle=True)\n",
        "    wordVecDimensions = wordVectors.shape[1]\n",
        "else:\n",
        "    question = 'Since we cant find an embedding matrix, how many dimensions do you want your word vectors to be?: '\n",
        "    wordVecDimensions = int(input(question))\n",
        "\n",
        "# Add two entries to the word vector matrix. One to represent padding tokens,\n",
        "# and one to represent an end of sentence token\n",
        "padVector = np.zeros((1, wordVecDimensions), dtype='int32')\n",
        "EOSVector = np.ones((1, wordVecDimensions), dtype='int32')\n",
        "if (os.path.isfile(path + 'embeddingMatrix.npy')):\n",
        "    wordVectors = np.concatenate((wordVectors,padVector), axis=0)\n",
        "    wordVectors = np.concatenate((wordVectors,EOSVector), axis=0)\n",
        "\n",
        "# Need to modify the word list as well\n",
        "wordList.append('<pad>')\n",
        "wordList.append('<EOS>')\n",
        "vocabSize = vocabSize + 2\n",
        "\n",
        "if (os.path.isfile(path + 'Seq2SeqXTrain.npy') and os.path.isfile(path + 'Seq2SeqYTrain.npy')):\n",
        "    xTrain = np.load(path + 'Seq2SeqXTrain.npy')\n",
        "    yTrain = np.load(path + 'Seq2SeqYTrain.npy')\n",
        "    print ('Finished loading training matrices')\n",
        "    numTrainingExamples = xTrain.shape[0]\n",
        "else:\n",
        "    numTrainingExamples, xTrain, yTrain = createTrainingMatrices(path + 'conversationDictionary.npy', wordList, maxEncoderLength)\n",
        "    np.save(path + 'Seq2SeqXTrain.npy', xTrain)\n",
        "    np.save(path + 'Seq2SeqYTrain.npy', yTrain)\n",
        "    print ('Finished creating training matrices')\n",
        "\n",
        "tf.compat.v1.reset_default_graph()\n",
        "\n",
        "# Create the placeholders\n",
        "encoderInputs = [tf.placeholder(tf.int32, shape=(None,)) for i in range(maxEncoderLength)]\n",
        "decoderLabels = [tf.placeholder(tf.int32, shape=(None,)) for i in range(maxDecoderLength)]\n",
        "decoderInputs = [tf.placeholder(tf.int32, shape=(None,)) for i in range(maxDecoderLength)]\n",
        "feedPrevious = tf.placeholder(tf.bool)\n",
        "\n",
        "#encoderLSTM = tf.nn.rnn_cell.BasicLSTMCell(lstmUnits, state_is_tuple=True)\n",
        "#2 СТРОКИ НИЖЕ заменяют одну выше\n",
        "singleCell = tf.nn.rnn_cell.BasicLSTMCell(lstmUnits, state_is_tuple=True)\n",
        "encoderLSTM = tf.nn.rnn_cell.MultiRNNCell([singleCell]*numLayersLSTM, state_is_tuple=True)\n",
        "# Architectural choice of of whether or not to include ^\n",
        "\n",
        "decoderOutputs, decoderFinalState = tf.contrib.legacy_seq2seq.embedding_rnn_seq2seq(encoderInputs, decoderInputs, encoderLSTM,vocabSize, vocabSize, embeddingDim, feed_previous=feedPrevious)\n",
        "\n",
        "decoderPrediction = tf.argmax(decoderOutputs, 2)\n",
        "\n",
        "lossWeights = [tf.ones_like(l, dtype=tf.float32) for l in decoderLabels]\n",
        "loss = tf.contrib.legacy_seq2seq.sequence_loss(decoderOutputs, decoderLabels, lossWeights, vocabSize)\n",
        "optimizer = tf.train.AdamOptimizer(1e-4).minimize(loss)\n",
        "\n",
        "sess = tf.Session()\n",
        "saver = tf.train.Saver()\n",
        "# If you're loading in a saved model, uncomment the following line and comment out line 205\n",
        "saver.restore(sess, tf.train.latest_checkpoint(path + 'models/'))\n",
        "#sess.run(tf.global_variables_initializer())\n",
        "\n",
        "# Uploading results to Tensorboard\n",
        "tf.summary.scalar('Loss', loss)\n",
        "merged = tf.summary.merge_all()\n",
        "logdir = path + \"tensorboard/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\") + \"/\"\n",
        "writer = tf.summary.FileWriter(logdir, sess.graph)\n",
        "\n",
        "# Some test strings that we'll use as input at intervals during training\n",
        "encoderTestStrings = [\"Здарова\",\n",
        "            \"Дарова\",\n",
        "            \"Привет\",\n",
        "            \"Хай\",\n",
        "            \"Как дела?\"\n",
        "            ]\n",
        "\n",
        "zeroVector = np.zeros((1), dtype='int32')\n",
        "\n",
        "for i in range(numIterations):\n",
        "\n",
        "    encoderTrain, decoderTargetTrain, decoderInputTrain = getTrainingBatch(xTrain, yTrain, batchSize, maxEncoderLength)\n",
        "    feedDict = {encoderInputs[t]: encoderTrain[t] for t in range(maxEncoderLength)}\n",
        "    feedDict.update({decoderLabels[t]: decoderTargetTrain[t] for t in range(maxDecoderLength)})\n",
        "    feedDict.update({decoderInputs[t]: decoderInputTrain[t] for t in range(maxDecoderLength)})\n",
        "    feedDict.update({feedPrevious: False})\n",
        "\n",
        "    curLoss, _, pred = sess.run([loss, optimizer, decoderPrediction], feed_dict=feedDict)\n",
        "\n",
        "    if (i % 50 == 0):\n",
        "        output.clear()\n",
        "        print('Current loss:', curLoss, 'at iteration', i)\n",
        "        summary = sess.run(merged, feed_dict=feedDict)\n",
        "        writer.add_summary(summary, i)\n",
        "    \n",
        "    if (i % 25 == 0 and i != 0):\n",
        "        num = randint(0,len(encoderTestStrings) - 1)\n",
        "        print (encoderTestStrings[num])\n",
        "        inputVector = getTestInput(encoderTestStrings[num], wordList, maxEncoderLength);\n",
        "        feedDict = {encoderInputs[t]: inputVector[t] for t in range(maxEncoderLength)}\n",
        "        feedDict.update({decoderLabels[t]: zeroVector for t in range(maxDecoderLength)})\n",
        "        feedDict.update({decoderInputs[t]: zeroVector for t in range(maxDecoderLength)})\n",
        "        feedDict.update({feedPrevious: True})\n",
        "        ids = (sess.run(decoderPrediction, feed_dict=feedDict))\n",
        "        print (idsToSentence(ids, wordList))\n",
        "\n",
        "    if (i % 10000 == 0 and i != 0):\n",
        "        savePath = saver.save(sess, path + \"models/pretrained_seq2seq.ckpt\", global_step=i)"
      ],
      "metadata": {
        "id": "ygN2SpBHsZeZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5f7c3b7-0e6d-4ba9-8747-1bcea6d83eee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current loss: 1.8501498 at iteration 3700\n",
            "Как дела?\n",
            "['1 ', 'я не хочу ', 'не уверен 1 ', 'так не знаю ']\n"
          ]
        }
      ]
    }
  ]
}